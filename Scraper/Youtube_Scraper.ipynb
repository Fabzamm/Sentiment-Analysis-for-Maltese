{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Youtube Comment Scraper"
      ],
      "metadata": {
        "id": "HDQJZzj8jsNT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and Setup"
      ],
      "metadata": {
        "id": "CyGPcRvMkQ2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "from googleapiclient.discovery import build"
      ],
      "metadata": {
        "id": "nYWOQsHJMJKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Simple time code remover\n",
        "import re\n",
        "\n",
        "def remove_timecodes(text):\n",
        "    # Pattern for h:mm:ss, mm:ss, h.mm.ss, mm.ss â€” all optionally prefixed with @ or @\n",
        "    timecode_pattern = r'(?<!\\w)(?:@\\s*)?(?:\\d{1,2}[:.]){1,2}\\d{2}(?!\\w)'\n",
        "    return re.sub(timecode_pattern, '', text)"
      ],
      "metadata": {
        "id": "ZZFa3Jnj-qzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Channel Search"
      ],
      "metadata": {
        "id": "ZQBtM89TkrcX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KenrtHHoR6wR"
      },
      "outputs": [],
      "source": [
        "# API Key\n",
        "api_key = ''\n",
        "\n",
        "# Username of channel\n",
        "channel_username = 'JonMalliaPodcast'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YouTube Data API request to search for the channel ID based on the username\n",
        "response = requests.get(\n",
        "    'https://www.googleapis.com/youtube/v3/search',\n",
        "    params={\n",
        "        'part': 'snippet',\n",
        "        'q': channel_username,  # Search for the channel name\n",
        "        'type': 'channel',\n",
        "        'key': api_key\n",
        "    }\n",
        ")\n",
        "\n",
        "response_json = response.json()\n",
        "\n",
        "# Extracting the channel ID if found, raisin error otherwise\n",
        "if 'items' in response_json and len(response_json['items']) > 0:\n",
        "    channel_id = response_json['items'][0]['id']['channelId']\n",
        "else:\n",
        "    print(\"Error: Unable to find channel.\")\n",
        "    print(response_json)  # Debugging\n",
        "    raise ValueError(\"Invalid channel username or API response.\")"
      ],
      "metadata": {
        "id": "TnNV1KgEgupa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Video ID and Comment Extraction Functions"
      ],
      "metadata": {
        "id": "LE2UU3-zlCkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_video_ids(channel_id, api_key):\n",
        "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
        "    video_ids = []\n",
        "    next_page_token = None\n",
        "\n",
        "    # Get the playlist ID for the channel's uploads\n",
        "    channel_response = youtube.channels().list(\n",
        "        part=\"contentDetails\",\n",
        "        id=channel_id\n",
        "    ).execute()\n",
        "    uploads_playlist_id = channel_response['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
        "\n",
        "    # Fetch all video IDs from the uploads playlist\n",
        "    while True:\n",
        "        playlist_response = youtube.playlistItems().list(\n",
        "            part=\"contentDetails\",\n",
        "            playlistId=uploads_playlist_id,\n",
        "            maxResults=50,\n",
        "            pageToken=next_page_token\n",
        "        ).execute()\n",
        "\n",
        "        for item in playlist_response['items']:\n",
        "            video_ids.append(item['contentDetails']['videoId'])\n",
        "\n",
        "        next_page_token = playlist_response.get('nextPageToken')\n",
        "        if not next_page_token:\n",
        "            break\n",
        "\n",
        "    return video_ids"
      ],
      "metadata": {
        "id": "aSjbgHwlgIG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_comments(video_id, api_key, max_comments=100):\n",
        "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
        "    comments = []\n",
        "    next_page_token = None\n",
        "\n",
        "    while len(comments) < max_comments:\n",
        "        request = youtube.commentThreads().list(\n",
        "            part=\"snippet\",\n",
        "            videoId=video_id,\n",
        "            textFormat=\"plainText\",\n",
        "            maxResults=100,  # Fetching 100 comments per request\n",
        "            pageToken=next_page_token\n",
        "        )\n",
        "        try:\n",
        "            response = request.execute()\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching comments for video {video_id}: {e}\")\n",
        "            return comments  # Return an empty list if an error occurs\n",
        "\n",
        "        for item in response['items']:\n",
        "            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
        "            cleaned = remove_timecodes(comment)\n",
        "            comments.append({\"content\": cleaned})\n",
        "            if len(comments) >= max_comments:\n",
        "                break\n",
        "\n",
        "        next_page_token = response.get('nextPageToken')\n",
        "        if not next_page_token:\n",
        "            break\n",
        "\n",
        "    return comments"
      ],
      "metadata": {
        "id": "S7V3HcMwgle6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comment Scraping Loop"
      ],
      "metadata": {
        "id": "myP8sEsrky6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetching all video IDs from the channel\n",
        "video_ids = get_all_video_ids(channel_id, api_key)\n",
        "\n",
        "# Fetching comments from each video and storing them in a list\n",
        "all_comments = []\n",
        "for video_id in video_ids:\n",
        "    print(f\"Fetching comments for video: {video_id}\")\n",
        "    try:\n",
        "        comments = get_comments(video_id, api_key, max_comments=1000)\n",
        "        all_comments.extend(comments)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing video {video_id}: {e}\")\n",
        "        continue  # Skip to the next video if an error occurs\n"
      ],
      "metadata": {
        "id": "OwxVAn_CL-WY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the results"
      ],
      "metadata": {
        "id": "RTCpj6Hjjy_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving as JSON file\n",
        "with open(f'{channel_username}.json', 'w', encoding='utf-8') as json_file:\n",
        "    json.dump(all_comments, json_file, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(\"Comments exported to youtube_channel_comments.json\")"
      ],
      "metadata": {
        "id": "mUjfs3Odghn3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}