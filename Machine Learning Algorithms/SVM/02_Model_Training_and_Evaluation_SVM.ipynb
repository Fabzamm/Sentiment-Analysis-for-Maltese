{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a78f58d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "import preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97080f2",
   "metadata": {},
   "source": [
    "### Evaluating the Original Jerbarnes Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7986c7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Evaluation on Original 'jerbarnes' Dataset Only ---\n",
      "Original 'jerbarnes' dataset loaded successfully. Samples: 851\n",
      "Jerbarnes Training set size: 680 samples\n",
      "Jerbarnes Test set size: 171 samples\n",
      "Starting GridSearchCV for SVM with various feature representations...\n",
      "Fitting 5 folds for each of 720 candidates, totalling 3600 fits\n",
      "\n",
      "Best parameters found for SVM on 'jerbarnes' data:\n",
      "{'classifier__C': 10, 'classifier__gamma': 'scale', 'classifier__kernel': 'rbf', 'vectorizer': TfidfVectorizer(lowercase=False, min_df=2), 'vectorizer__ngram_range': (1, 1), 'vectorizer__norm': 'l2', 'vectorizer__use_idf': True}\n",
      "\n",
      "Best cross-validation F1-weighted score for SVM on 'jerbarnes' data:\n",
      "0.734102027825346\n",
      "\n",
      "Classification Report for Best SVM Model (Optimized & Tested on 'jerbarnes' only):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.93      0.85       117\n",
      "           1       0.74      0.43      0.54        54\n",
      "\n",
      "    accuracy                           0.77       171\n",
      "   macro avg       0.76      0.68      0.69       171\n",
      "weighted avg       0.77      0.77      0.75       171\n",
      "\n",
      "Accuracy for Best SVM Model (Optimized & Tested on 'jerbarnes' only): 0.7719\n",
      "\n",
      "--- End of Evaluation on Original 'jerbarnes' Dataset Only ---\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path('./data')\n",
    "\n",
    "\n",
    "print(\"--- Starting Evaluation on Original 'jerbarnes' Dataset Only ---\")\n",
    "\n",
    "try:\n",
    "    # Load only the original 'jerbarnes' dataset\n",
    "    jerbarnes_dataset_path = DATA_DIR/'jerbarnes_dataset_selective_lowercased_lemmatized.csv'\n",
    "    df_jerbarnes = pd.read_csv(jerbarnes_dataset_path, header=None, names=['label', 'text', 'processed_text'])\n",
    "    print(f\"Original 'jerbarnes' dataset loaded successfully. Samples: {len(df_jerbarnes)}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading 'jerbarnes' data: {e}. Please ensure the file exists at {jerbarnes_dataset_path}\")\n",
    "    exit()\n",
    "\n",
    "# Splitting the 'jerbarnes' dataset into training and testing sets (80/20 split)\n",
    "X_old_train, X_old_test, y_old_train, y_old_test = train_test_split(\n",
    "    df_jerbarnes['processed_text'], df_jerbarnes['label'],\n",
    "    test_size=0.2, random_state=42, stratify=df_jerbarnes['label']\n",
    ")\n",
    "\n",
    "print(f\"Jerbarnes Training set size: {len(X_old_train)} samples\")\n",
    "print(f\"Jerbarnes Test set size: {len(X_old_test)} samples\")\n",
    "\n",
    "\n",
    "# Defining the SVM pipeline for hyperparameter tuning (using CountVectorizer)\n",
    "svm_pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()), # Placeholder for vectorizer, will be replaced by GridSearchCV\n",
    "    ('classifier', SVC(probability=True, random_state=42)), # probability=True is needed for predict_proba\n",
    "])\n",
    "\n",
    "# Defining the hyperparameter grid for SVM\n",
    "param_grid = [\n",
    "    {\n",
    "        'vectorizer': [CountVectorizer(min_df=2, lowercase=False, strip_accents=None, stop_words=None)],\n",
    "        'vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "        'classifier__C': [0.1, 1, 10, 100], # Regularization parameter\n",
    "        'classifier__kernel': ['linear', 'rbf'], # Specifies the kernel type\n",
    "        'classifier__gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1], # Kernel coefficient for 'rbf', 'poly' and 'sigmoid'\n",
    "    },\n",
    "    {\n",
    "        'vectorizer': [TfidfVectorizer(min_df=2, lowercase=False, strip_accents=None, stop_words=None)],\n",
    "        'vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "        'vectorizer__use_idf': [True, False],\n",
    "        'vectorizer__norm': ['l1', 'l2'],\n",
    "        'classifier__C': [0.1, 1, 10, 100], # Regularization parameter\n",
    "        'classifier__kernel': ['linear', 'rbf'], # Specifies the kernel type\n",
    "        'classifier__gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1], # Kernel coefficient for 'rbf', 'poly' and 'sigmoid'\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"Starting GridSearchCV for SVM with various feature representations...\")\n",
    "grid_search_svm_old = GridSearchCV(svm_pipeline, param_grid, cv=5, verbose=1, n_jobs=-1, scoring='f1_weighted')\n",
    "grid_search_svm_old.fit(X_old_train, y_old_train)\n",
    "\n",
    "print(\"\\nBest parameters found for SVM on 'jerbarnes' data:\")\n",
    "print(grid_search_svm_old.best_params_)\n",
    "print(\"\\nBest cross-validation F1-weighted score for SVM on 'jerbarnes' data:\")\n",
    "print(grid_search_svm_old.best_score_)\n",
    "\n",
    "# Evaluating the best SVM model (optimized on jerbarnes) on the 'jerbarnes' test set\n",
    "best_svm_old_model = grid_search_svm_old.best_estimator_\n",
    "y_pred_old_svm = best_svm_old_model.predict(X_old_test)\n",
    "\n",
    "print(\"\\nClassification Report for Best SVM Model (Optimized & Tested on 'jerbarnes' only):\")\n",
    "print(classification_report(y_old_test, y_pred_old_svm))\n",
    "print(f\"Accuracy for Best SVM Model (Optimized & Tested on 'jerbarnes' only): {accuracy_score(y_old_test, y_pred_old_svm):.4f}\")\n",
    "\n",
    "print(\"\\n--- End of Evaluation on Original 'jerbarnes' Dataset Only ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65e4b4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the results for the final report\n",
    "report_model_a_dict = classification_report(y_old_test, y_pred_old_svm, output_dict=True)\n",
    "accuracy_model_a = accuracy_score(y_old_test, y_pred_old_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686a37f8",
   "metadata": {},
   "source": [
    "### Adding Crowd-sourced data to training and re-evaluating on the old test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc41b364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Evaluation with Crowd-sourced Data Added to Training ---\n",
      "Crowd-sourced dataset loaded successfully. Samples: 1594\n",
      "Extended Training set size (Jerbarnes train + Crowd-sourced): 2274 samples\n",
      "\n",
      "Retraining the best SVM model (from jerbarnes optimization) on the EXTENDED training data...\n",
      "Retraining complete.\n",
      "\n",
      "Classification Report for SVM Model (Retrained on Extended Data, Tested on 'jerbarnes' only):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.89      0.83       117\n",
      "           1       0.66      0.46      0.54        54\n",
      "\n",
      "    accuracy                           0.75       171\n",
      "   macro avg       0.72      0.68      0.69       171\n",
      "weighted avg       0.74      0.75      0.74       171\n",
      "\n",
      "Accuracy for SVM Model (Retrained on Extended Data, Tested on 'jerbarnes' only): 0.7544\n",
      "\n",
      "--- End of Evaluation with Crowd-sourced Data Added to Training ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Starting Evaluation with Crowd-sourced Data Added to Training ---\")\n",
    "\n",
    "try:\n",
    "    # Loading the crowd-sourced dataset\n",
    "    crowdsourced_dataset_path = DATA_DIR/'crowdsourced_dataset_selective_lowercased_lemmatized.csv'\n",
    "    df_crowdsourced = pd.read_csv(crowdsourced_dataset_path, header=None, names=['label', 'text', 'processed_text'])\n",
    "    print(f\"Crowd-sourced dataset loaded successfully. Samples: {len(df_crowdsourced)}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading crowd-sourced data: {e}. Please ensure the file exists at {crowdsourced_dataset_path}\")\n",
    "    exit()\n",
    "\n",
    "# Combining the jerbarnes training set with the entire crowd-sourced dataset\n",
    "X_extended_train = pd.concat([X_old_train, df_crowdsourced['processed_text']], ignore_index=True)\n",
    "y_extended_train = pd.concat([y_old_train, df_crowdsourced['label']], ignore_index=True)\n",
    "\n",
    "print(f\"Extended Training set size (Jerbarnes train + Crowd-sourced): {len(X_extended_train)} samples\")\n",
    "\n",
    "# Retraining the BEST SVM model (found from jerbarnes-only optimization) on the extended training set\n",
    "print(\"\\nRetraining the best SVM model (from jerbarnes optimization) on the EXTENDED training data...\")\n",
    "retrained_svm_extended = best_svm_old_model.fit(X_extended_train, y_extended_train)\n",
    "print(\"Retraining complete.\")\n",
    "\n",
    "# Evaluating this newly retrained model on the SAME 'jerbarnes' test set\n",
    "y_pred_extended_svm = retrained_svm_extended.predict(X_old_test)\n",
    "\n",
    "print(\"\\nClassification Report for SVM Model (Retrained on Extended Data, Tested on 'jerbarnes' only):\")\n",
    "print(classification_report(y_old_test, y_pred_extended_svm))\n",
    "print(f\"Accuracy for SVM Model (Retrained on Extended Data, Tested on 'jerbarnes' only): {accuracy_score(y_old_test, y_pred_extended_svm):.4f}\")\n",
    "\n",
    "print(\"\\n--- End of Evaluation with Crowd-sourced Data Added to Training ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f80c4e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_model_b_dict = classification_report(y_old_test, y_pred_extended_svm, output_dict=True)\n",
    "accuracy_model_b = accuracy_score(y_old_test, y_pred_extended_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd8970a",
   "metadata": {},
   "source": [
    "### Evaluation on the full combined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecd4d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Evaluation on the Full Combined Dataset (Optimized & Tested on original jerbarnes test set) ---\n",
      "Combined Training set size for Model C: 2445 samples\n",
      "Test set size (original jerbarnes test set) for Model C: 171 samples\n",
      "\n",
      "Starting GridSearchCV for SVM on the FULL Combined Dataset (for Model C optimization)...\n",
      "Fitting 5 folds for each of 720 candidates, totalling 3600 fits\n",
      "\n",
      "Best parameters found for SVM on the FULL Combined Dataset (for Model C):\n",
      "{'classifier__C': 10, 'classifier__gamma': 'scale', 'classifier__kernel': 'rbf', 'vectorizer': TfidfVectorizer(lowercase=False, min_df=2), 'vectorizer__ngram_range': (1, 1), 'vectorizer__norm': 'l2', 'vectorizer__use_idf': True}\n",
      "\n",
      "Best cross-validation F1-weighted score for SVM on the FULL Combined Dataset (for Model C):\n",
      "0.7681495895075671\n",
      "\n",
      "Classification Report for Best SVM Model (Optimized on Combined Data, Tested on original 'jerbarnes' only):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.89      0.83       117\n",
      "           1       0.66      0.46      0.54        54\n",
      "\n",
      "    accuracy                           0.75       171\n",
      "   macro avg       0.72      0.68      0.69       171\n",
      "weighted avg       0.74      0.75      0.74       171\n",
      "\n",
      "Accuracy for Best SVM Model (Optimized on Combined Data, Tested on original 'jerbarnes' only): 0.7544\n",
      "\n",
      "--- End of Evaluation on the Full Combined Dataset (Optimized & Tested on original jerbarnes test set) ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Starting Evaluation on the Full Combined Dataset (Optimized & Tested on original jerbarnes test set) ---\")\n",
    "\n",
    "# Using the original jerbarnes test set for evaluation of Model C as well\n",
    "X_test_for_C = X_old_test\n",
    "y_test_for_C = y_old_test\n",
    "\n",
    "print(f\"Combined Training set size for Model C: {len(X_combined_train_for_C)} samples\")\n",
    "print(f\"Test set size (original jerbarnes test set) for Model C: {len(X_test_for_C)} samples\")\n",
    "\n",
    "\n",
    "# Defining the SVM pipeline for hyperparameter tuning\n",
    "svm_pipeline_combined_C = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()), # Placeholder for vectorizer\n",
    "    ('classifier', SVC(probability=True, random_state=42)),\n",
    "])\n",
    "\n",
    "# Defining the hyperparameter grid for SVM with vectorizer optimization \n",
    "param_grid_combined_C = [\n",
    "    {\n",
    "        'vectorizer': [CountVectorizer(min_df=2, lowercase=False, strip_accents=None, stop_words=None)],\n",
    "        'vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "        'classifier__C': [0.1, 1, 10, 100],\n",
    "        'classifier__kernel': ['linear', 'rbf'],\n",
    "        'classifier__gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    "    },\n",
    "    {\n",
    "        'vectorizer': [TfidfVectorizer(min_df=2, lowercase=False, strip_accents=None, stop_words=None)],\n",
    "        'vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "        'vectorizer__use_idf': [True, False],\n",
    "        'vectorizer__norm': ['l1', 'l2'],\n",
    "        'classifier__C': [0.1, 1, 10, 100],\n",
    "        'classifier__kernel': ['linear', 'rbf'],\n",
    "        'classifier__gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"\\nStarting GridSearchCV for SVM on the FULL Combined Dataset (for Model C optimization)...\")\n",
    "grid_search_svm_combined_C = GridSearchCV(svm_pipeline_combined_C, param_grid_combined_C, cv=5, verbose=1, n_jobs=-1, scoring='f1_weighted')\n",
    "grid_search_svm_combined_C.fit(X_extended_train, y_extended_train) # Fit on the full combined training data\n",
    "\n",
    "print(\"\\nBest parameters found for SVM on the FULL Combined Dataset (for Model C):\")\n",
    "print(grid_search_svm_combined_C.best_params_)\n",
    "print(\"\\nBest cross-validation F1-weighted score for SVM on the FULL Combined Dataset (for Model C):\")\n",
    "print(grid_search_svm_combined_C.best_score_)\n",
    "\n",
    "# Evaluating the best SVM model (optimized on combined data) on the ORIGINAL JERBARNES test set\n",
    "best_svm_combined_model_C = grid_search_svm_combined_C.best_estimator_\n",
    "y_pred_combined_svm_C = best_svm_combined_model_C.predict(X_test_for_C) # Predict on X_old_test\n",
    "\n",
    "print(\"\\nClassification Report for Best SVM Model (Optimized on Combined Data, Tested on original 'jerbarnes' only):\")\n",
    "print(classification_report(y_test_for_C, y_pred_combined_svm_C))\n",
    "print(f\"Accuracy for Best SVM Model (Optimized on Combined Data, Tested on original 'jerbarnes' only): {accuracy_score(y_test_for_C, y_pred_combined_svm_C):.4f}\")\n",
    "\n",
    "print(\"\\n--- End of Evaluation on the Full Combined Dataset (Optimized & Tested on original jerbarnes test set) ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92b98c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_model_c_dict = classification_report(y_test_for_C, y_pred_combined_svm_C, output_dict=True)\n",
    "accuracy_model_c = accuracy_score(y_test_for_C, y_pred_combined_svm_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7a7f70",
   "metadata": {},
   "source": [
    "### Printing the Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43118012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SVM Model Performance Comparison ---\n",
      "   Class/Avg    Metric  Model A  Model B  Model C\n",
      "           0 precision     0.78     0.78     0.78\n",
      "           0    recall     0.93     0.89     0.89\n",
      "           0  f1-score     0.85     0.83     0.83\n",
      "           1 precision     0.74     0.66     0.66\n",
      "           1    recall     0.43     0.46     0.46\n",
      "           1  f1-score     0.54     0.54     0.54\n",
      "     Overall  Accuracy     0.77     0.75     0.75\n",
      "   macro avg precision     0.76     0.72     0.72\n",
      "   macro avg    recall     0.68     0.68     0.68\n",
      "   macro avg  f1-score     0.69     0.69     0.69\n",
      "weighted avg precision     0.77     0.74     0.74\n",
      "weighted avg    recall     0.77     0.75     0.75\n",
      "weighted avg  f1-score     0.75     0.74     0.74\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def extract_metrics(report_dict, accuracy):\n",
    "    metrics = {\n",
    "        '0': {\n",
    "            'precision': report_dict['0']['precision'],\n",
    "            'recall': report_dict['0']['recall'],\n",
    "            'f1-score': report_dict['0']['f1-score'],\n",
    "        },\n",
    "        '1': {\n",
    "            'precision': report_dict['1']['precision'],\n",
    "            'recall': report_dict['1']['recall'],\n",
    "            'f1-score': report_dict['1']['f1-score'],\n",
    "        },\n",
    "        'macro avg': {\n",
    "            'precision': report_dict['macro avg']['precision'],\n",
    "            'recall': report_dict['macro avg']['recall'],\n",
    "            'f1-score': report_dict['macro avg']['f1-score'],\n",
    "        },\n",
    "        'weighted avg': {\n",
    "            'precision': report_dict['weighted avg']['precision'],\n",
    "            'recall': report_dict['weighted avg']['recall'],\n",
    "            'f1-score': report_dict['weighted avg']['f1-score'],\n",
    "        },\n",
    "        'Overall': {\n",
    "            'Accuracy': accuracy\n",
    "        }\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# Extract metrics for each model\n",
    "metrics_model_a = extract_metrics(report_model_a_dict, accuracy_model_a)\n",
    "metrics_model_b = extract_metrics(report_model_b_dict, accuracy_model_b)\n",
    "metrics_model_c = extract_metrics(report_model_c_dict, accuracy_model_c)\n",
    "\n",
    "# Preparing data for DataFrame\n",
    "data = []\n",
    "for class_label in ['0', '1', 'Overall', 'macro avg', 'weighted avg']:\n",
    "    if class_label == 'Overall':\n",
    "        row_accuracy = {\n",
    "            'Class/Avg': class_label,\n",
    "            'Metric': 'Accuracy',\n",
    "            'Model A': metrics_model_a[class_label].get('Accuracy'),\n",
    "            'Model B': metrics_model_b[class_label].get('Accuracy'),\n",
    "            'Model C': metrics_model_c[class_label].get('Accuracy')\n",
    "        }\n",
    "        data.append(row_accuracy)\n",
    "    else:\n",
    "        for metric_name in ['precision', 'recall', 'f1-score']:\n",
    "            row = {\n",
    "                'Class/Avg': class_label,\n",
    "                'Metric': metric_name,\n",
    "                'Model A': metrics_model_a[class_label].get(metric_name),\n",
    "                'Model B': metrics_model_b[class_label].get(metric_name),\n",
    "                'Model C': metrics_model_c[class_label].get(metric_name)\n",
    "            }\n",
    "            data.append(row)\n",
    "\n",
    "# Create the DataFrame\n",
    "comparison_df = pd.DataFrame(data)\n",
    "\n",
    "# Print the table (adjust formatting if needed)\n",
    "print(\"\\n--- SVM Model Performance Comparison ---\")\n",
    "print(comparison_df.to_string(index=False, float_format=\"%.2f\"))\n",
    "\n",
    "\n",
    "comparison_df.to_csv('svm_performance_comparison.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
